{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sexual-accused",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "Classification is a supervised machine learning algorithm which categorically assigns data based on predefined classes.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-hammer",
   "metadata": {},
   "source": [
    "In the most common scenario, classes are taken to be disjoint, so that each input is assigned to one and only one class. Therefore, the input spaces is divided into regions we call $decision \\ regions$ and whose boundaries we call $decision \\ boundardies$ or $decision \\ sur\\textit{f}aces$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-consequence",
   "metadata": {},
   "source": [
    "Here consider only models in which the decision surfaces are linear functions of the input vector $x$ and hence are defined by $(D-1)$-dim hyperplanes within the $D$-dim input space.\n",
    "\n",
    "We define these sets whose classes can be sperated exactly by linear decision surfaces as $linearly \\ seperable$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-threshold",
   "metadata": {},
   "source": [
    "In the case of classifaction, there a various ways of using target values to represent class labels. \n",
    "\n",
    "For probabilistic models, the most convinient, in the case of two-class problems, is the binary representation. In this there is a single target variable $t\\in\\{0,1\\}$ such that $t=1$ respresnts class $C_1$ and $t=0$ respresnts class $C_2$. We can interpret the value of $t$ as the probability that the class is $C_1$, with values of proability only taking the extreme values of $0$ and $1$. \n",
    "For $K>2$ classes, it is convinient to use a $1$-of-$K$ coding scheme in which $\\mathbf{t}$ is a vector of length $K$ such that if the class is $C_j$, the all the elements $t_k$ of $\\mathbf{t}$ are zero except element $t_j$, which take the value $1$. For instance, if we have $K=5$ classes, then a pattern from class $C_2$ would be given the target vector $\\mathbf{t} = (0,1,0,0,0)^T$. For nonprobabilitstic models, alternative choice of target variable representation will sometimes prove convenient.\n",
    "\n",
    "Again, <font color = green>we interpret  the value of $t_k$ as the probability that the class is $C_k$</font>. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-universal",
   "metadata": {},
   "source": [
    "Here we introduce three distinct approaches to the classification problem. \n",
    "1. The simplest involves constructing a $discriminant \\ \\textit{f}unction$ that directly assigns each vector $\\vec{x}$ to a specific class. \n",
    "\n",
    "A more powerful apporach, is to model the conditional probability distribution $p(C_k|\\mathbf{\\vec{x}})$ in an inference stage, and the subsequently use this distribution to make optimal decisions. By seperating inference and decision, we gain numerous benefits. \n",
    "However, there are two appraches to determining our conditional probaility distribution.\n",
    "\n",
    "2. Model $p(C_k|\\mathbf{\\vec{x}})$ directy. For example, by representing them as parametric models and then optimizing the parameters using a training set.\n",
    "\n",
    "\n",
    "3. Take on a generative approach in computing the posterior proability $p(C_k|\\mathbf{\\vec{x}})$. By modelling the class-conditional densities given by $p(\\mathbf{\\vec{x}}|C_k)$, and modeling the prior probabilities $p(C_k)$, we can use Bayes theorem calculate the posterior.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-median",
   "metadata": {},
   "source": [
    "For classification problems, we wish to predict discrete class labels, or more generally, posterior probabilities that lie in the range $(0,1)$. To achieve this, we consider a generaliztion of this model in which we transform the linear function of $\\vec{w}$ using a nonlinear function $f(\\cdot)$ so that\n",
    "\n",
    "$$\\color{green}{y(\\mathbf{\\vec{x}}) = f(\\vec{w}^T\\mathbf{\\vec{x}} + w_0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-biotechnology",
   "metadata": {},
   "source": [
    "Our nonlinear function $f(\\cdot)$ is known as an $activation \\ \\textit{f}unction$ (ML), whereas its inverse is called a $link \\ \\textit{f}unction$ (Stats).\n",
    "\n",
    "The decision surfaces correspond to $y(\\mathbf{\\vec{x}}) = $ constant, so that $\\vec{w}^T\\mathbf{\\vec{x}} + w_0 = $ constant. Hence, the decision surfaces are linear functions of $\\mathbf{\\vec{x}}$, even if the activation function is nonlinear. However, the activation function makes it so that the model is no longer in the parameters $\\vec{w}$.\n",
    "\n",
    "The above equation describes a class of models called $generalized\\ linear\\ models$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-referral",
   "metadata": {},
   "source": [
    "The following algorithms will be equally applicable if we first make a fixed nonlinear transformation of the input variables using a vector of basis functions $\\phi(\\vec{\\mathbf{x}})$\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-integral",
   "metadata": {},
   "source": [
    "# Discriminant Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-richardson",
   "metadata": {},
   "source": [
    "A $discriminant\\ function$ is a function that takes an input vector $\\vec{x}$ and assign it to one of $K$ discrete classes $C_k$.\n",
    "\n",
    "For this section we will restrict our attention to $linear$ discriminants, namely those for which the decision boundaries are hypyerplanes\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-canyon",
   "metadata": {},
   "source": [
    "#### Two Classes\n",
    "To simplify the discussion, we consider first the case of $K=2$ classes and then investigate the extension to $K > 2$ classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-vacation",
   "metadata": {},
   "source": [
    "The simplest representation of a linear discriminant function is obtained by taking a linear function of the input vector so that\n",
    "\n",
    "$$y(\\mathbf{\\vec{x}}) = \\vec{w}^T\\mathbf{\\vec{x}} + w_0 $$\n",
    "where $\\vec{w}$ is called the $weight\\ vector$, and $w_0$ is a $bias$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-leather",
   "metadata": {},
   "source": [
    "An input vector $\\mathbf{\\vec{x}}$ is assinged to class\n",
    "- $C_1$ if $y(\\mathbf{\\vec{x}}) \\geq0$\n",
    "- $C_2$ otherwise\n",
    "Therefore, the corresponding decision boundary is defined by the relation\n",
    "$y(\\mathbf{\\vec{x}})=0$.\n",
    "\n",
    "Remember that this decision boundary corresponds to a $(D-1)$-dim hypyerplane within the $D$-dim input space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-printing",
   "metadata": {},
   "source": [
    "Consider two points $\\mathbf{\\vec{x_a}}$ and $\\mathbf{\\vec{x_b}}$ both of which lie on the decision surface.\n",
    "\n",
    "Because $y(\\mathbf{\\vec{x_a}})  = y(\\mathbf{\\vec{x_b}})=0$, we have that \n",
    "$\\vec{w}^T(\\mathbf{\\vec{x_a}}-\\mathbf{\\vec{x_b}})=0$.\n",
    "\n",
    "Hence, $\\vec{w}$ is orthogonal to every vector lying within the decision surface, and so $\\vec{w}$ determines the orientation of the decision surface. Similarly, if $\\mathbf{\\vec{x}}$ is a point on the decision surface, than $y(\\mathbf{\\vec{x}}) = 0$, and so the normal distance from the origin to decision surface is given by \n",
    "\n",
    "$$\\dfrac{\\vec{w}^T\\mathbf{\\vec{x}}}{||\\vec{w}||} = -\\dfrac{w_0}{||\\vec{w}||}$$\n",
    "\n",
    "Thus the bias parameter $w_0$ determines the location of the decision surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d1c40-26e2-455c-a7b2-74f5f437f023",
   "metadata": {},
   "source": [
    "Furthermore, we note that the value of $y(\\mathbf{\\vec{x}})$ gives a signifigant measure of the perependicular distance $r$ of the point $\\mathbf{\\vec{x}}$ from the decision surface\n",
    "\n",
    "Now, consider an arbitrary point $\\mathbf{\\vec{x}}$ and let $\\mathbf{\\vec{x}}_{\\perp}$ be its orthogoanl projection onto the decision surface so that \n",
    "\n",
    "$$\\mathbf{\\vec{x}} = \\mathbf{\\vec{x}}_{\\perp} +r\\dfrac{\\vec{w}}{||\\vec{w}||}$$\n",
    "\n",
    "Multiplying both sides of the result by $\\vec{w}^T$ and adding $w_0$, and making use of $y(\\mathbf{\\vec{x}}) = \\vec{w}^T\\mathbf{\\vec{x}} + w_0 $ and $y(\\mathbf{\\vec{x}_{\\perp}}) = \\vec{w}^T\\mathbf{\\vec{x}_{\\perp}} + w_0 = 0$, we have\n",
    "\n",
    "$$r = \\dfrac{y(\\mathbf{\\vec{x}})}{||\\vec{w}||} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-beads",
   "metadata": {},
   "source": [
    "Let us visualize these relationships by restricting the input space to 2 dimensions $x_1$ and $x_2$. Here we graph just one input vector $\\vec{x}$. It would be better if included another vector $\\vec{x_b}$ and relabeled $\\vec{x}$ to $\\vec{x_a}$, however, this still gives all the neccesary information. We can intricately see how the weight vector controls the decision surface in the input space, and how the decision surface categorizes data based on its location in the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-blank",
   "metadata": {},
   "source": [
    "<img src='./Figures/LinearClassificationFigure1.png'\n",
    "     style=\"width:280px;height:221px;\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-tribe",
   "metadata": {},
   "source": [
    "As with linear regression models, it is sometimes convinient to use a more compact notation in which we introduce a 'dummy' input value $x_0=1$ and then define $\\tilde{\\vec{w}} = (w_0,\\vec{w})$ and $\\tilde{\\mathbf{\\vec{x}}} = (x_0,\\mathbf{\\vec{x}})$, so that \n",
    "$y(\\mathbf{\\vec{x}}) =\\tilde{\\vec{w}} \\ ^T \\tilde{\\mathbf{\\vec{x}}}$. In this case, the decision surfaces are $D$-dim hyperplanes passing through the origin of the $(D + 1)$-dim expanded input space.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-infrared",
   "metadata": {},
   "source": [
    "#### Multiple Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79139391-3fc9-4c8c-b4d1-e28dff68c947",
   "metadata": {},
   "source": [
    "Now consider the extension of linear discriminants to $K > 2$ classes. We might be tempted be to build a K-class discriminant by combining a number of two-class discriminant functions. However, this leads to some serious difficulties as we will show."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6527db-f6ad-4713-8152-a4e79af27e59",
   "metadata": {},
   "source": [
    "Consider a single K-class discriminant comprising K linear functions of the form \n",
    "\n",
    "$$y_{k}(\\mathbf{\\vec{x}}) = \\vec{w}_{k}^T\\mathbf{\\vec{x}} + w_{k0}$$\n",
    "\n",
    "and then assigning a point $\\mathbf{\\vec{x}}$ to class $C_k$ if $y_k(\\mathbf{\\vec{x}})>y_j(\\mathbf{\\vec{x}})$ for all $j\\neq k$. The decision boundary between class $C_k$ aqnd class $C_j$ is there given by $y_k(\\mathbf{\\vec{x}})=y_j(\\mathbf{\\vec{x}})$ and hence corresponds to a $(D-1)$-dim hyperplane defined by\n",
    "\n",
    "$$( \\vec{w}_{k}- \\vec{w}_{j})^T\\mathbf{\\vec{x}}( w_{k0} - w_{j0} )$$\n",
    "\n",
    "This has the same form as the decision boundary for the two-class cade discussed above, and so analygous geometric properties apply. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df54ff5-52a2-4136-a52c-6b9103b9a38a",
   "metadata": {},
   "source": [
    "The decision regions of such a discriminant are always singly connected and convex. To see this, consider two points $\\vec{x}_A$ and  $\\vec{x}_B$ both of which lie inside decision region $R_k$. Any point $\\hat{\\vec{x}}$ that lies on the line connecting $\\vec{x}_A$ and $\\vec{x}_B$ can be expressed in the form\n",
    "\n",
    "$$\\hat{\\vec{x}} = \\lambda\\vec{x}_A + (1-\\lambda)\\vec{x}_B$$\n",
    "\n",
    "where $ 0 \\geq \\lambda \\geq 1$. From the linearity of the discriminant functions, it follows that \n",
    "\n",
    "$$y_{k}(\\hat{\\vec{x}}) = \\lambda y_{k}(\\vec{x}_A) + (1-\\lambda) y_{k}(\\vec{x}_B)$$\n",
    "\n",
    "Because both $\\vec{x}_A$ and $\\vec{x}_B$ lie inside $R_k$, it follows that $y_{k}(\\vec{x}_A)>y_{j}(\\vec{x}_A)$, and $y_{k}(\\vec{x}_B)>y_{j}(\\vec{x}_B)$ for all $j \\neq k$, and hence $y_{k}(\\hat{\\vec{x}})>y_{j}(\\hat{\\vec{x}})$, and so $\\hat{\\vec{x}}$ also inside $R_k$. Thus, $R_k$ is singly connected and convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304b870-f75b-4489-b4a5-f91c19d4669a",
   "metadata": {},
   "source": [
    "Note that for two classes, we can either employ the method discussed here based on two discriminant functions $y_{1}(\\mathbf{\\vec{x}})$ and $y_{2}(\\mathbf{\\vec{x}})$, or we can use the simpler, yet equivalent formula above based on a single discriminant function $y(\\mathbf{\\vec{x}})$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d1ab4-42af-44bf-ad76-2d2fcb5e1769",
   "metadata": {},
   "source": [
    "We now explore only one of three approaches to learning the parameters of linear discriminant functions- based on least squares - out of based on least squares, Fisher’s linear discriminant, and the perceptron algorithm. We will return to the others further in the discussion.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a274ebd-7656-4e14-8db2-239087f54f8a",
   "metadata": {},
   "source": [
    "## Least Squares for Classification\n",
    "\n",
    "Consider a general classification problem with $K$ classes, with a $1$-of-$K$ binary coding scheme for the target vector $\\mathbf{\\vec{t}}$.\n",
    "\n",
    "One justification for using least squares in such a context is that it approximates the conditional expectation $E[\\mathbf{\\vec{t}}|\\mathbf{\\vec{x}}]$ of the target values given the input vector. For the binary coding scheme, this conditional expectation is given by the vector of posterior class probabilities. Unfortunately, however, these probabilities are typically approximated rather poorly, indeed the approximations can have values outside the range $(0, 1)$, due to the limited flexibility of a linear model as we shall see shortly.\n",
    "\n",
    "Each class $C_k$ is decribed by its own linear model so that\n",
    "\n",
    "$$y_{k}(\\mathbf{\\vec{x}}) = \\vec{w}_{k}^T\\mathbf{\\vec{x}} + w_{k0}$$\n",
    "\n",
    "where $k = 1,\\dots,K$. We can group these together so that\n",
    "\n",
    "$$y_{k}(\\mathbf{\\vec{x}}) = \\widetilde{\\mathbf{W}}^T\\tilde{\\mathbf{\\vec{x}}}$$\n",
    "\n",
    "where $\\widetilde{\\mathbf{W}}$ is a matrix whose $k^{th}$ column comprises the $(D+1)$-dim vecor $\\tilde{\\vec{w_k}} = (w_{k0},\\vec{w_k}^T)^T$ and $\\tilde{\\mathbf{\\vec{x}}}$ is the corresponding augmented input vecor $(1,\\mathbf{\\vec{x}}^T)^T$ with a dummy input $x_0 = 1$. A new input $\\mathbf{\\vec{x}}$ is then assigned to the class for which the output $y_k = \\tilde{\\vec{w_k}}^T\\tilde{\\mathbf{\\vec{x}}}$ is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf3f227-c7e3-4bb7-90b4-d96cf961b2db",
   "metadata": {},
   "source": [
    "Now, we determing the parameter matrix  $\\widetilde{\\mathbf{W}}$  by minimizing the sum-of-squares error function.\n",
    "\n",
    "Consider a training data set $\\{\\vec{x_n},\\vec{t_n}\\}$ where $n=1,\\dots,N$, and define a matrix $\\mathbf{T}$ whose $nth$ row is the vector $\\vec{t_n}$,\n",
    "together with a matrix $\\tilde{\\mathbf{X}}$ whose $nth$ row is $\\tilde{\\vec{x_{n}}}^T$ . The sum-of-squares error function can then be written as\n",
    "\n",
    "$$E_D(\\widetilde{\\mathbf{W}}) = \\dfrac{1}{2} Tr\\{ \\ (\\tilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\mathbf{T})^T(\\tilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\mathbf{T}) \\ \\}$$\n",
    "\n",
    "Setting the derivative with respect to $\\widetilde{\\mathbf{W}}$ to zero, and rearanging, we obtain the folliwng solution\n",
    "\n",
    "$$\\widetilde{\\mathbf{W}} = (\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}})^{-1}\\tilde{\\mathbf{X}}^T\\mathbf{T} $$\n",
    "\n",
    "Or we can write it in terms of the Moore-Penrose psuedo inverse $\\tilde{\\mathbf{X}}^{\\dagger}$ so that we have \n",
    "\n",
    "$$\\widetilde{\\mathbf{W}} = \\tilde{\\mathbf{X}}^{\\dagger}\\mathbf{T}$$\n",
    "\n",
    "We then obtain the dicriminant function in the form \n",
    "\n",
    "$$y(\\mathbf{\\vec{x}}) = \\widetilde{\\mathbf{W}}^T\\tilde{\\mathbf{\\vec{x}}} =  \\mathbf{T}^T( \\ \\tilde{\\mathbf{X}}^{\\dagger})^T\\tilde{\\mathbf{\\vec{x}}}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb96330-3d94-489d-8e3a-1e84b9a34915",
   "metadata": {},
   "source": [
    "An interesting property of least-squares solutions with multiple target variables is that if every target vector in the training set satisfies some linear constraint\n",
    "\n",
    "$$\\vec{a}^T \\vec{t_n} + b = 0$$\n",
    "\n",
    "for some constants $\\vec{a}$ and $b$, then the model predicition for any value of $\\mathbf{\\vec{x}}$ will satisfy the same constraint so that\n",
    "\n",
    "$$ \\vec{a}^T y(\\mathbf{\\vec{x}}) + b = 0 $$\n",
    "\n",
    "Thus, if we usa a $1$-of-$K$ coding scheme for $K$ classes, then the predictions made by the model will have the property that the elements of $y(\\mathbf{\\vec{x}})$ will sum to 1 for any value of $\\mathbf{\\vec{x}}$. However, this summation contraint alone is not sufficient to allow the model outputs to be interpreted as probabilities because they are not contrained to lie in the interval (0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa89773-e7fd-4eef-b954-ee08843f9da0",
   "metadata": {},
   "source": [
    "The least-squares approach gives an exact closed-form solution for the discrimi- nant function parameters. However, it suffers from some severe problems.\n",
    "\n",
    "1. Lack of robustness to outliers: The sum-of-squares error function penalizes predictions that are 'too correct' in that they lie far away from, but on the correct side, of the decision boundary. In the example given, points in the bottom right of the input space shift the decision boundary,which lies in the upper left corner, downward. This shift causes points that were correctly labelled near the decision boundary to become incorrectly labelled - they now lie on the other side.\n",
    "\n",
    "2. The least squares method corresponds to maximum likelihood under the assumption of a Gaussian conditional distribution. Here, our $binary$ target vectors clearly have a distibution that is farm from Gaussian, so it can lead to poor results easily.By adopting more appropriate probabilistic models, we shall obtain classification techniques with much better properties than least squares. However, there are also alternative nonprobabilistic methods for setting the parameters, which we shall continue with later\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-visiting",
   "metadata": {},
   "source": [
    "Let us define a $logistic \\ sigmoid \\ \\textit{f}unction \\ \\sigma(a)$  given by\n",
    "\n",
    "$$ \\sigma(a) = \\dfrac{1}{1+\\exp(-a)} .$$\n",
    "\n",
    "It satisfies the following symmetry property\n",
    "\n",
    "$$\\sigma(-a) = 1- \\sigma(a)$$\n",
    "\n",
    "by way of \n",
    "\n",
    "$$ 1 - \\sigma(a) = 1 - \\dfrac{1}{1 + 1/\\exp(a)} = \\dfrac{\\exp(a)+1}{\\exp(a)+1} - \\dfrac{\\exp(a)}{\\exp(a) + 1} = \\dfrac{1}{1+\\exp(a)} .$$ \n",
    "\n",
    "The inverse is known as the $logit$ function and is given by\n",
    "\n",
    "$$ a = \\ln (\\dfrac{\\sigma}{1-\\sigma})$$\n",
    "\n",
    "by way of\n",
    "\n",
    "$$  \\dfrac{1}{\\sigma} - 1 = \\dfrac{1}{\\exp(a)} \\ \\ --> \\ \\  \\dfrac{1 - \\sigma}{\\sigma} = \\dfrac{1}{\\exp(a)} \\ \\ --> \\ \\ \\exp(a) = \\dfrac{\\sigma}{1 - \\sigma}   $$\n",
    "\n",
    "\n",
    "and the derivative is given by \n",
    "\n",
    "$$ \\dfrac{d\\sigma}{da} = \\sigma(1-\\sigma)  $$\n",
    "\n",
    "by way of \n",
    "\n",
    "$$ \\dfrac{d\\sigma}{da} = \\dfrac{-1 * -\\exp(-a)}{(1 + \\exp(-a))^2}  = \\dfrac{1}{1 + \\exp(-a)}*\\dfrac{\\exp(-a)}{1 + \\exp(-a)}\n",
    "= \\sigma * \\dfrac{\\exp(-a)*\\exp(a)}{1+\\exp(a)} = \\sigma (1-\\sigma)    . $$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867aea04-04cd-429c-93b1-af62787c6d00",
   "metadata": {},
   "source": [
    "# Probablistic Generative Models\n",
    "We now turn to a probabilistic view of classification and show how models with linear decision boundaries arise from simple assumptions about the distribution of the data.\n",
    "\n",
    "Here we shall adopt a generative approach in which we model the class-conditional densities $p(\\mathbf{\\vec{x}}|C_k)$, as well as the class priors $p(C_k)$, and then use these to compute posterior probabilities $p(C_k|\\mathbf{\\vec{x}})$ through Bayes’ theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-entity",
   "metadata": {},
   "source": [
    "#### Two Classes\n",
    "\n",
    "Consider first of all the case of two classes. The posterior probability for class $C_1$ can be written as\n",
    "$$p(C_1|\\mathbf{\\vec{x}}) \n",
    "= \\dfrac{p(\\mathbf{\\vec{x}}|C_1)p(C_1)}{p(\\mathbf{\\vec{x}}|C_1)p(C_1)+p(\\mathbf{\\vec{x}}|C_2)p(C_2)} $$\n",
    "$$ = \\dfrac{1}{1+\\exp(-a)} = \\sigma(a) $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ a = \\ln \\dfrac{p(\\mathbf{\\vec{x}}|C_1)p(C_1)}{p(\\mathbf{\\vec{x}}|C_2)p(C_2)} $$\n",
    "\n",
    "Here, in the case of two classes, the logit function represents the log of the ratio of probailities $ln[p(C_1|\\mathbf{\\vec{x}})/p(C_2|\\mathbf{\\vec{x}})]$for the two classes, also known as the $log \\ odds$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-bernard",
   "metadata": {},
   "source": [
    "#### Multiple Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7487881c-e2fb-49ab-a2de-192b1677ebc0",
   "metadata": {},
   "source": [
    "The posterior probability for class $C_k$ is given by \n",
    "\n",
    "$$p(C_k|\\mathbf{\\vec{x}}) = \\dfrac{p(\\mathbf{\\vec{x}}|C_k)p(C_k)}{\\sum_j p(\\mathbf{\\vec{x}}|C_j)p(C_j)} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-chancellor",
   "metadata": {},
   "source": [
    "Now, let us extend our logit function to apply it to classification. Let us define $a_k$ as\n",
    "\n",
    "$$a_k = \\ln p(\\mathbf{\\vec{x}}|C_k)p(C_k)$$\n",
    "\n",
    "This allows us to rewrite our equation above, called the $normalized \\ exponential$, given by\n",
    "\n",
    "$$p(C_k|\\mathbf{\\vec{x}})  = \\dfrac{\\exp(a_k)}{\\sum_j \\exp(a_j)} $$\n",
    "\n",
    "Although putting the equation in this form makes little sense now, we will see the reasoning for it later on.\n",
    "\n",
    "The normalized exponential is also know as the $so\\textit{f}tmax \\ \\textit{f}unction$, as it represents a smoothed version of the 'max' function because, if $a_k \\gg a_j$ for all $j\\neq k$, then $p(C_k|\\mathbf{\\vec{x}})\\simeq 1$ and $p(C_j|\\mathbf{\\vec{x}})\\simeq 0$.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e50ba3-7ad2-4f2a-884a-d08e941891b3",
   "metadata": {},
   "source": [
    "We now investigate the consequences of choosing specific forms for the class-conditional densities, looking first at continuous input variables and then discussing briefly the case of discrete inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539884bc-80b5-456e-aa05-ff464c59aec1",
   "metadata": {},
   "source": [
    "## Continuous Inputs\n",
    "\n",
    "Let us assume that the class-conditional densities are Gaussian and then explore the resulting form for the posterior probabilities. To start with, we shall assume that all classes share the same covariance matrix. Thus the density for class C_k is given by\n",
    "\n",
    "$$p(\\mathbf{\\vec{x}}|C_k) = \\dfrac{1}{(2\\pi)^{D/2}}\\dfrac{1}{|\\mathbf{\\Sigma}|^{1/2}} \\exp \\biggl\\{-\\dfrac{1}{2}(\\mathbf{\\vec{x}}-\\vec{\\mu_k})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{\\vec{x}}-\\vec{\\mu_k}) \\biggl\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63b554-ae68-4e78-9c6a-75bf13e78c59",
   "metadata": {},
   "source": [
    "Here we have that\n",
    "\n",
    "$$a_k(\\mathbf{\\vec{x}}) = \\vec{w_k}^T\\mathbf{\\vec{x}} + w_{k0}$$\n",
    "\n",
    "where we have defined\n",
    "$$ \\vec{w_k} =\\mathbf{\\Sigma}^{-1}\\vec{\\mu_k}$$\n",
    "\n",
    "$$w_{k0} = -\\dfrac{1}{2}\\vec{\\mu_k}^T\\mathbf{\\Sigma}^{-1}\\vec{\\mu_k} + \\ln p(C_k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f06c2-daf9-48dc-9993-93e0871e696d",
   "metadata": {},
   "source": [
    "We see that the $a_k(\\mathbf{\\vec{x}})$ are again linear functions of $\\mathbf{\\vec{x}}$ as a consequence of the cancellation of the quadratic terms due to the shared covariances.\n",
    "\n",
    "The resulting decision boundaries, corresponding to the minimum misclassification rate, will occur when two of the posterior probabilities (the two largest) are equal, and so will be defined by linear functions of  $\\mathbf{\\vec{x}}$. Thus, we have a generalized linear model.\n",
    "\n",
    "If we relax the assumption of a shared covariance matrix and allow each class conditional density $p(\\mathbf{\\vec{x}}|C_k)$ to have it own covariance matrix $\\mathbf{\\Sigma_k}$, then the earlier cancellations will no longer occur. In this case, we obtain quadratic functions of $\\mathbf{\\vec{x}}$, giving rise to a $quadratic \\ discriminant$. We can visualize these results below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295e01d-4586-495e-a992-311f3067f260",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src='./Figures/LinearClassificationFigure2.png'style=\"width:280px;height:280px;\">\n",
    "    <img src='./Figures/LinearClassificationFigure3.png'\n",
    "     style=\"width:280px;height:280px;\"\n",
    "     desc = \"pp\" >\n",
    "    <figcaption  style='font-size: 10px; text-align: center;'>The left-hand plot shows the class-conditional densities for three classes each having a Gaussian distribution, coloured red, green, and blue, in which the red and green classes have the same covariance matrix. The right-hand plot shows the corresponding posterior probabilities, in which the RGB colour vector represents the posterior probabilities for the respective three classes. The decision boundaries are also shown. Notice that the boundary between the red and green classes, which have the same covariance matrix, is linear, whereas those between the other pairs of classes are quadratic.</figcaption>\n",
    "</figure>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e84b62-62bc-45f7-8274-97c3e8039cde",
   "metadata": {},
   "source": [
    "## Maximum Likelihood\n",
    "Once we have specified a parametric functional form for the class-conditional densities $p(\\mathbf{\\vec{x}})$, we can then determine the values of the parameters, together with the prior class probabilites $p(C_k)$, using maximum likelihood.\n",
    "\n",
    "This requires a data set comprising observations of $\\mathbf{\\vec{x}}$ along with their corresponding class labels.\n",
    "\n",
    "#### Two classes\n",
    "Consider first the case of two classes, each having a Gaussian class-conditional density with a shared covariance matrix, and suppose we have a data set $\\{\\vec{\\mathbf{x_n}},\\mathbf{t_n}\\}$ where $n = 1,\\dots,N$ Here $\\mathbf{t_n} = 1$ denotes class $C_1$ and $\\mathbf{t_n} = 0$ denotes class $C_2$. \n",
    "We denote the prior class probability $p(C_1) = \\pi$, so that $p(C_2) = 1 − \\pi$. For a data point $\\vec{\\mathbf{x_n}}$ from class $C_1$, we have $\\mathbf{t_n} = 1$ and hence\n",
    "\n",
    "$$p(\\vec{\\mathbf{x_n}},C_1) = p(C_1)p(\\vec{\\mathbf{x_n}}|C_1) = \\pi N(\\vec{\\mathbf{x_n}}|\\vec{\\mu_1},\\mathbf{\\Sigma}) $$\n",
    "\n",
    "Similarly, for class $C_2$ we have $\\mathbf{t_n} = 0$ and hence\n",
    "\n",
    "$$ p(\\vec{\\mathbf{x_n}},C_2) = p(C_2)p(\\vec{\\mathbf{x_n}}|C_2) = (1-\\pi)N(\\vec{\\mathbf{x_n}}|\\vec{\\mu_2},\\mathbf{\\Sigma})$$\n",
    "\n",
    "Thus the likelihood function is given by \n",
    "\n",
    "$$p(\\mathbf{t}|\\pi,\\vec{\\mu_1},\\vec{\\mu_2},\\mathbf{\\Sigma}) = \\prod_{n=1}^{N} [\\pi N(\\vec{x_n}|\\vec{\\mu_1},\\mathbf{\\Sigma})]^{t_n} [1-\\pi)N(\\vec{x_n}|\\vec{\\mu_2},\\mathbf{\\Sigma})]^{1 - t_n}$$\n",
    "\n",
    "where $\\mathbf{t} = (t_1,\\dots,t_N)^T$.\n",
    "As usual, it is convinient to maximize the log of the likelihood function. Consider first the maximization with respect to $\\pi$. The terms in the log likelihood function that depend on $\\pi$ are \n",
    "\n",
    "$$\\sum_{n=1}^{N} \\{t_n \\ln \\pi  \\ + \\ (1-t_n)\\ln (1-\\pi) \\}$$\n",
    "\n",
    "Setting the derivative with respect to $\\pi$ equal to zero and rearranging, we obtain\n",
    "\n",
    "$$\\pi = \\dfrac{1}{N} \\sum_{n=1}^{N} t_n =  \\dfrac{N_1}{N} =  \\dfrac{N_1}{N_1 + N_2}$$\n",
    "\n",
    "where $N_k$ dentoes the total number of data points in class $C_k$. Thus, the maximum likelihood estimate for $\\pi$ is simply the fraction of points in class $C_1$ as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9a775-b3c3-41ab-9569-29983aa928f5",
   "metadata": {},
   "source": [
    "Now consider the maximization with respect to $\\vec{\\mu_1}$. Again, we can pick out of the log likelihood function those terms that depend on $\\vec{\\mu_1}$, giving\n",
    "\n",
    "$$\\sum_{n=1}^{N} t_n \\ln [ N(\\vec{x_n}|\\vec{\\mu_1},\\mathbf{\\Sigma})  ] = - \\dfrac{1}{2}\\sum_{n=1}^{N} t_n (\\vec{x_n} - \\vec{\\mu_1})^T \\mathbf{\\Sigma}^{-1} (\\vec{x_n} - \\vec{\\mu_1}) + const. $$\n",
    "\n",
    "Setting the derivative with respect to $\\vec{\\mu_1}$ to zero and rearranging, we obtain\n",
    "\n",
    "$$\\vec{\\mu_1} = \\dfrac{1}{N_1}\\sum_{n=1}^{N} t_n \\vec{x_n} $$\n",
    "\n",
    "which is simply the mean of all the input vectors $\\vec{x_n}$ assinged to class $C_1$. By a similar argument, the correspond result for $\\vec{\\mu_2}$ is given by \n",
    "\n",
    "$$\\vec{\\mu_2} = \\dfrac{1}{N_2}\\sum_{n=1}^{N}(1- t_n) \\vec{x_n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441f901-1fb2-4d7f-a8f2-cb55602dbb48",
   "metadata": {},
   "source": [
    "Finally, consider the maximum likelihood solution for the shared covariance matrix $\\mathbf{\\Sigma}$. Picking out the terms in the log likelihood function that depend on $\\mathbf{\\Sigma}$, we have \n",
    "\n",
    "$$ - \\dfrac{1}{2}\\sum_{n=1}^{N} t_n \\ln |\\mathbf{\\Sigma}| - \\dfrac{1}{2}\\sum_{n=1}^{N} t_n (\\vec{x_n} - \\vec{\\mu_1})^T \\mathbf{\\Sigma}^{-1} (\\vec{x_n} - \\vec{\\mu_1}) - \\dfrac{1}{2}\\sum_{n=1}^{N}(1- t_n)\\ln  |\\mathbf{\\Sigma}| $$\n",
    "$$ - \\dfrac{1}{2}\\sum_{n=1}^{N} (1- t_n)(\\vec{x_n} - \\vec{\\mu_2})^T \\mathbf{\\Sigma}^{-1} (\\vec{x_n} - \\vec{\\mu_2}) $$\n",
    "\n",
    "$$ = -\\dfrac{N}{2}\\ln |\\mathbf{\\Sigma}| -\\dfrac{N}{2} Tr \\{ \\mathbf{\\Sigma}^{-1} \\mathbf{S} \\}$$\n",
    "\n",
    "\n",
    "where we have defined \n",
    "\n",
    "$$\\mathbf{S} = \\dfrac{N_1}{N}\\mathbf{S_1} + \\dfrac{N_2}{N}\\mathbf{S_2} $$\n",
    "\n",
    "$$\\mathbf{S_1} = \\dfrac{1}{N_1} \\sum_{n\\in C_1} (\\vec{x_n} - \\vec{\\mu_1})(\\vec{x_n} - \\vec{\\mu_1})^T $$\n",
    "\n",
    "$$\\mathbf{S_2} = \\dfrac{1}{N_2} \\sum_{n\\in C_2} (\\vec{x_n} - \\vec{\\mu_2})(\\vec{x_n} - \\vec{\\mu_2})^T $$\n",
    "\n",
    "We can use the standard result for the maximum likelihood solution for a Gaussian distribution, to see that $\\mathbf{S}$ respresents a weighted average of the covariance matrices associated with each of the two classes seperately. Hence, we have\n",
    "\n",
    "$$\\mathbf{\\Sigma}=\\mathbf{S}  = \\dfrac{N_1}{N}\\mathbf{S_1} + \\dfrac{N_2}{N}\\mathbf{S_2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f09ee-a408-4201-af33-fa31edb21040",
   "metadata": {},
   "source": [
    "#### Multiple classes\n",
    "The results above can easily be extended to the K class problem to obtain the corresponding maximium likelihood solutions for the parameters in which case each class-conditional density is Gaussian with a shared covariance matrix. \n",
    "\n",
    "Given \n",
    "\n",
    "$$p(C_j) = \\pi_j$$\n",
    "$$p(\\phi_n|C_j) = N(\\phi_n|\\vec{\\mu_j},\\mathbf{\\Sigma})$$\n",
    "\n",
    "by the product rule we have that\n",
    "\n",
    "$$ p(\\phi_n,C_j) = \\pi N(\\phi_n|\\vec{\\mu_j},\\mathbf{\\Sigma})$$\n",
    "\n",
    "likelihood function\n",
    "\n",
    "$$p(\\mathbf{t}, \\Phi |\\pi,\\vec{\\mu},\\mathbf{\\Sigma}) = \\prod_{n=1}^N \\prod_{j=1}^K [\\pi_j N(\\phi_n|\\vec{\\mu_j},\\mathbf{\\Sigma})]^{t_{nj}} $$\n",
    "\n",
    "log likelihood\n",
    "\n",
    "$$ -\\dfrac{1}{2}\\sum_{n=1}^{N}\\sum_{j=1}^{K} t_{nj} [K + \\ln |\\mathbf{\\Sigma}| + (\\vec{\\phi_n} - \\vec{\\mu_j})^T \\mathbf{\\Sigma}^{-1} (\\vec{\\phi_n} - \\vec{\\mu_j})]   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af14b5-a595-4218-bf17-bdacf5224e22",
   "metadata": {},
   "source": [
    "maximum likelihood estimate for $\\pi$ is simply the fraction of points in the class as expected\n",
    "\n",
    "$$\\pi_k = \\dfrac{N_k}{N}$$\n",
    "\n",
    "for each class mean we have\n",
    "\n",
    "$$ \\vec{\\mu_k} = \\dfrac{1}{N} \\sum_{n=1}^{N} t_{nk} \\phi_n $$ \n",
    "\n",
    "and finally for the shared covariance matrix we get\n",
    "\n",
    "$$\\mathbf{\\Sigma} = \\sum_{k=1}^{K} \\dfrac{N_k}{N} \\mathbf{S_k} $$\n",
    "where \n",
    "$$ \\mathbf{S_k} = \\dfrac{1}{N_k} \\sum_{n=1}^{N} t_{nk}(\\phi_n - \\vec{\\mu_k})(\\phi_n - \\vec{\\mu_k})^T $$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81d1081-f9a0-49b2-ab07-a33e91a8fb60",
   "metadata": {},
   "source": [
    "\n",
    "Note that the approach of fitting Gaussian distributions to the classes is not robust to outliers, because the maximum likelihood estimation of a Gaussian is not robust.\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-quilt",
   "metadata": {},
   "source": [
    "# Probabilistic Discriminative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-desperate",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-behalf",
   "metadata": {},
   "source": [
    "#### Two Classes\n",
    "We begin our treatment of generalized linear models by considering the problem of two-class classification. In our discussion of generative approaches, we saw that under rather general assumptions, the posterior probability of class $C_1$ can be written as a logistic sigmoid acting on a linear function of the feature vector $\\phi$ so that\n",
    "\n",
    "$$p(C_1|\\phi) = y(\\phi) = \\sigma(\\vec{w}^T \\phi)$$\n",
    "\n",
    "with $p(C_1|\\phi) = 1 - p(C_2|\\phi)$.\n",
    "\n",
    "In the terminology of statistics, this model is known as $logistic \\ regression$.\n",
    "\n",
    "For an $M$-dimensional feature space $\\phi$, this model has $M$ adjustable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-auction",
   "metadata": {},
   "source": [
    "We now use maximum likelihood to determine the parameters of the logistic regression model. To do this, we shall make use of the derivative of the logistic sigmoid function. \n",
    "\n",
    "For a data set $\\{\\phi_n,t_n\\}$, where $t_n \\in \\{0,1\\}$ and $\\phi_n = \\phi(\\mathbf{\\vec{x_n}})$, with $n =1,\\dots,N$, the likelihood function can be written\n",
    "\n",
    "$$ p(\\mathbf{t}|\\vec{w}) = \\prod_{n=1}^{N} y_n^{t_n} \\{1-y_n\\}^{1-t_n}  $$\n",
    "\n",
    "where $\\mathbf{t} = (t_1 , \\dots, t_N)^T$ and $y_n = p(C_1|\\phi_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-appreciation",
   "metadata": {},
   "source": [
    "As usual, we can define an error function by taking the negative log of the likelihood, which gives us the $cross\\textit{-}entropy$  error function of the form\n",
    "\n",
    "$$ E(\\vec{w}) = - \\ln p(\\mathbf{t}|\\vec{w}) = - \\sum_{n=1}^{N} \\{t_n \\ln  y_n + (1-t_n)\\ln (1- y_n) \\} $$\n",
    "\n",
    "where $y_n = \\sigma(a_n)$ and $a_n = \\vec{w}^T \\phi_n$.\n",
    "\n",
    "Taking the gradient of the error function with respect to $\\vec{w}$ we obtain\n",
    "\n",
    "$$\\nabla E(\\vec{w}) = \\sum_{n=1}^{N} (y_n - t_n)\\phi_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-kitty",
   "metadata": {},
   "source": [
    "This error function can be minimized by an efficient iterative technique based on the $Newton-Raphson$ iterative optimization scheme, which uses a local quadratic approximation to the log likelihood function.\n",
    "\n",
    "The Newton-Raphson update for minimzing a function $E(\\vec{w})$ takes the form\n",
    "\n",
    "$$\\vec{w}^{(new)} =  \\vec{w}^{(old)} - \\mathbf{H}^{-1} \\nabla E(\\vec{w}) $$\n",
    "\n",
    "where $\\mathbf{H}$ is the Hessian matrix whose elements comprise the second derivatives of $E(\\vec{w})$ with respect to the components of $\\vec{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-feeding",
   "metadata": {},
   "source": [
    "We see that the gradient is given by\n",
    "\n",
    "$$\\nabla E(\\vec{w}) = \\sum_{n=1}^{N} (y_n - t_n)\\phi_n = \\mathbf{\\Phi}^T (\\mathbf{y} -\\mathbf{t})$$\n",
    "\n",
    "And the Hessian of the error function is \n",
    "\n",
    "$$\\mathbf{H} = \\nabla \\nabla E(\\vec{w}) = \\nabla \\sum_{n=1}^{N} y_n\\phi_n = \\nabla \\sum_{n=1}^{N} \\sigma(\\vec{w}^T \\phi_n)\\phi_n $$\n",
    "$$= \\sum_{n=1}^{N} \\sigma(\\vec{w}^T \\phi_n) (1 - \\sigma(\\vec{w}^T \\phi_n))\\phi_n *\\phi_n^T = \\sum_{n=1}^{N} y_n(1-y_n) \\phi_n \\phi_n^T$$\n",
    "$$ = \\mathbf{\\Phi}^T \\mathbf{R} \\mathbf{\\Phi} $$\n",
    "\n",
    "where we drop $t_n$ from $\\nabla E(\\vec{w})$ due to its independence of $\\vec{w}$. We use the derivative of the sigmoid function above to get the desired results. Also, we have introduced the $NxN$ diagonal matrix $\\mathbf{R}$ with elements\n",
    "\n",
    "$$ R_{nn} = y_n(1-y_n) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-french",
   "metadata": {},
   "source": [
    "Here we have that the Hessian is not contstant. It depends on $\\vec{w}$ throught the weighting matrix $\\mathbf{R}$, corresponding to the fact that the error function is not quadratic.\n",
    "\n",
    "If we were to apply this techinque to linear regression we would find that $\\mathbf{H}=\\mathbf{\\Phi}^T\\mathbf{\\Phi}$ is constant, corresponding to the fact that the error function $\\nabla E(\\vec{w}) = \\mathbf{\\Phi}^T \\mathbf{\\Phi}\\vec{w}- \\mathbf{\\Phi}^T\\mathbf{t}$ is quadratic. In this case we can get the exact solution from the Newton-Raphson formula in one step, which gives the same standard least-squares solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-dublin",
   "metadata": {},
   "source": [
    "From the form of the sigmoid function we have the property\n",
    "$$0<y_n<1$$\n",
    "Hence, for an arbitrary $\\vec{\\mu}$, we have that\n",
    "\n",
    "$$\\vec{\\mu}^T\\mathbf{H}\\vec{\\mu} > 0$$ \n",
    "\n",
    "Thus $\\mathbf{H}$ is a positive definite, meaning the error function is a concave function of $\\vec{w}$ and has a unique minimum.\n",
    "\n",
    "The Newton-Raphson update for $E(\\vec{w})$ now takes the form\n",
    "\n",
    "$$\\vec{w}^{(new)} =  \\vec{w}^{(old)} -  (\\mathbf{\\Phi}^T \\mathbf{R} \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^T (\\mathbf{y} -\\mathbf{t})$$\n",
    "$$ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ =(\\mathbf{\\Phi}^T \\mathbf{R} \\mathbf{\\Phi})^{-1}\\{\\mathbf{\\Phi}^T \\mathbf{R} \\mathbf{\\Phi}\\vec{w}^{(old)} -  \\mathbf{\\Phi}^T (\\mathbf{y} -\\mathbf{t})\\}$$\n",
    "$$ =(\\mathbf{\\Phi}^T \\mathbf{R} \\mathbf{\\Phi})^{-1}\\mathbf{\\Phi}^T\\mathbf{R}\\mathbf{\\vec{z}}   \\ \\ \\ \\ \\ \\ \\ \\ $$\n",
    "\n",
    "where $\\mathbf{\\vec{z}}$ is an $N$-dim vector with elements \n",
    "\n",
    "$$ \\mathbf{\\vec{z}} = \\mathbf{\\Phi} \\vec{w}^{(old)} - \\mathbf{R}^{-1}(\\mathbf{y} -\\mathbf{t}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-jaguar",
   "metadata": {},
   "source": [
    "We see that the update formula (formula for $\\vec{w}^{(new)}$) takes the form of a set of normal equations for a weighted least-squares problem.\n",
    "\n",
    "Because the weighing matrix $\\mathbf{R}$ is not constant but depends on the parameter vector $\\vec{w}$, we must apply the normal equations iteratively, each time using the new weight vector $\\vec{w}$ to compute a revised weighing matrix $\\mathbf{R}$.\n",
    "\n",
    "Thus, this algorithm for minmizing the given error function, using the Newton-Raphson formula, is known as $itertive \\ reweighted \\ least \\ squares$ or $\\textit{I}RLS$\n",
    "\n",
    "As in the weighted least-squares problem, the elements of the diagonal wieghting matrix $\\mathbf{R}$ can be interpreted as variances. This is because the mean and variance of $t$ in the logistic regression model are given by\n",
    "\n",
    "$$E[t] = \\sigma(\\mathbf{\\vec{x}}) = y $$\n",
    "\n",
    "$$var[t] = E[t^2]-E[t]^2 = \\sigma(\\mathbf{\\vec{x}}) - \\sigma(\\mathbf{\\vec{x}})^2 = y(1-y) $$\n",
    "\n",
    "where we have used the fact $t^2=t $ for $t\\in\\{0,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-laser",
   "metadata": {},
   "source": [
    "We can interpret IRLS as the solution to a linearized problem in the space of the variable $a = \\vec{w}^T\\phi$. \n",
    "\n",
    "The quantity $z_n$, which corresponds to the nth element of $\\mathbf{\\vec{z}}$, can then be given a simple interpretation as an effective target value in this space obtained by making a local linear approximation to the logistic sigmoid function around the current operating point $\\vec{w}^{(old)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-arnold",
   "metadata": {},
   "source": [
    "$$a_n(\\vec{w}) \\simeq a_n(\\vec{w}^{(old)}) + \\dfrac{\\textit{d}a_n}{\\textit{d}y_n}\\Big|_{\\vec{w}^{(old)}} (t_n - y_n) $$\n",
    "\n",
    "$$  \\ \\ \\ \\ \\ \\ \\ = \\phi_n^T \\vec{w}^{(old)} - \\dfrac{(y_n - t_n)}{y_n(1-y_n)} = z_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-bulletin",
   "metadata": {},
   "source": [
    "#### Multiple Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-orange",
   "metadata": {},
   "source": [
    "In the case of generative models for multiclass classification, we have seen that for a large class of distributions, the posterior probabilities are given by a softmax transformation of linear functions of the feature variables, so that\n",
    "\n",
    "$$p(C_k|\\mathbf{\\vec{x}})  =y_k(\\phi)= \\dfrac{\\exp(a_k)}{\\sum_j \\exp(a_j)} $$\n",
    "\n",
    "where the ‘activations’ $a_k$ are given by\n",
    "\n",
    "$$ a_k = \\vec{w}_k^T \\phi $$\n",
    "\n",
    "There we used maximum likelihood to determine separately the class-conditional densities and the class priors and then found the corresponding posterior probabilities using Bayes’ theorem, thereby implicitly determining the parameters $\\{\\vec{w_k}\\}$.\n",
    "\n",
    "Here we consider the use of maximum likelihood to determine the parameters $\\{\\vec{w_k}\\}$ of this model directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-translation",
   "metadata": {},
   "source": [
    "The likelihood function is then given by\n",
    "\n",
    "$$p(\\mathbf{T}|\\vec{w_1},\\dots ,\\vec{w_K}) = \\prod_{n=1}^{N}\\prod_{k=1}^{K} p(C_k|\\phi_n)^{t_{nk}} = \\prod_{n=1}^{N}\\prod_{k=1}^{K} y_{nk}^{t_{nk}}$$\n",
    "\n",
    "where $y_{nk} = y_k(\\phi_n)$, and $\\mathbf{T}$ is an $N$ x $K$ matrix of target variables with elements $t_{nk}$\n",
    "\n",
    "As usual, we can define an error function by taking the negative log of the likelihood, which gives us the $cross\\textit{-}entropy$  error function for the multiclass classification problem of the form\n",
    "\n",
    "$$ E(\\vec{w_1},\\dots ,\\vec{w_K})= - \\sum_{n=1}^{N}\\sum_{k=1}^{K}t_{nk} \\ln y_{nk}$$\n",
    "\n",
    "We now take the gradient of the error function with respect to one of the parameter vectors $\\vec{w_j}$\n",
    "\n",
    "$$ \\nabla_{\\vec{w_j}} E(\\vec{w_1},\\dots ,\\vec{w_K})= \\sum_{n=1}^N (y_{nk} - t_{nj})\\phi_n$$\n",
    "\n",
    "where we have made use of $\\sum_k t_{nk} = 1$\n",
    "\n",
    "To find a batch algorithm, we again appeal to the Newton-Raphson update to obtain the corresponding IRLS algorithm for the multiclass problem. \n",
    "\n",
    "Here the Hessian matrix that comprises blocks of size M × M in which block (j, k) is given by\n",
    "\n",
    "$$ \\nabla_{\\vec{w_k}} \\nabla_{\\vec{w_j}}E(\\vec{w_1},\\dots ,\\vec{w_K}) = - \\sum_{n=1}^N y_{nk}(I_{kj} - y_{nj})\\phi_n\\phi_n^T $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-whale",
   "metadata": {},
   "source": [
    "To find this requires the derivatives of $y_k$ with respect to all of the activations $a_j$ . These are given by\n",
    "\n",
    "$$\\dfrac{\\partial y_k}{\\partial a_k} = \\dfrac{e^{a_k}}{\\sum_i e^{a_i}} - (\\dfrac{e^{a_k}}{\\sum_i e^{a_i}})^2 = \\dfrac{e^{a_k}}{\\sum_i e^{a_i}} (1 - \\dfrac{e^{a_k}}{\\sum_i e^{a_i}}) = y_k(1-y_k)   ,$$\n",
    "\n",
    "$$\\dfrac{\\partial y_k}{\\partial a_j} = - \\dfrac{e^{a_k}e^{a_j}}{(\\sum_i e^{a_i})^2} = \\dfrac{e^{a_k}}{\\sum_i e^{a_i}} \\dfrac{e^{a_j}}{\\sum_i e^{a_i}} = -y_k y_j, \\ \\ \\ \\ \\  j \\neq k$$\n",
    "\n",
    "Combining these results into a sinle equation, where $I_{kj}$ represents the elements of the identitiy matrix, we have\n",
    "\n",
    "$$\\dfrac{\\partial y_k}{\\partial a_j} = y_k(I_{kj} - y_j) $$\n",
    "\n",
    "giving the desired result above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-charter",
   "metadata": {},
   "source": [
    "As with the two-class problem, we the Hessian matrix here is a positive definitive and so the error function has a unique minimum\n",
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
