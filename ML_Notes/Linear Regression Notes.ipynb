{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e409ef9-c38e-469c-8e25-edb59f79ace0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear Regression\n",
    "Linear regression is a supervied machine learning algortihm which finds the optimal linear relationship between a target variable and its regressors (independent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09df19f-1ac2-4e55-9015-b33498bf6185",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1183e-b9d9-4e17-b8e6-9520252baa5c",
   "metadata": {},
   "source": [
    "Our goal is to predict the values of the target variable $\\vec{t}$ given M sets of data points for the values of $\\vec{t}$ and N regressors \n",
    "$\\{t_m,\\vec{x_{m1}},\\vec{x_{m2}},....,\\vec{x_{mN}}\\}_{j=0}^{M-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74452e2e-32bb-4dcd-8c61-77fec8499d16",
   "metadata": {},
   "source": [
    "Then, we have the model:\n",
    "$$ \\vec{t} = X\\vec{w} + \\vec{\\epsilon} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613eb88-8f13-45f2-b7d0-b74645c80ec3",
   "metadata": {},
   "source": [
    "Where $ \n",
    "\\vec{w} =\n",
    "\\begin{bmatrix}\n",
    "{w_0, w_1, \\dots, w_N}\n",
    "\\end{bmatrix}^T\n",
    "$ are the regression coefficents (paremeters), $\n",
    "\\vec{\\epsilon} =\n",
    "\\begin{bmatrix}\n",
    "{\\epsilon_{0}, \\epsilon_1, \\dots, \\epsilon_{M-1}}\n",
    "\\end{bmatrix}^T\n",
    "$ is the error term \n",
    ", and $\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "\\vec{x_{0n}}^T \\\\\n",
    "\\vec{x_{1n}}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\vec{x_{(M-1)n}}^T \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\ \\ \\ \\ \\ x_{11} \\  \\ \\ \\ \\ x_{12} \\dots  \\  \\ \\ \\ \\ x_{1N} \\\\\n",
    "1 \\  \\ \\ \\ \\ x_{21} \\  \\ \\ \\ \\ x_{22} \\dots  \\  \\ \\ \\ \\ x_{2N} \\\\\n",
    "\\vdots \\\\\n",
    "1 \\ \\ x_{(M-1)1} \\ \\ x_{(M-1)2} \\dots \\ \\ x_{(M-1)N}\n",
    "\\end{bmatrix}\n",
    "$\n",
    ". Note the first column in $X$ is to account for $w_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd3041-0b2d-4ce6-bc90-f9401be14b7f",
   "metadata": {},
   "source": [
    "In the most basic form of simple linear regression, we only have one regressor $x$. Hence, our model simplifies to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0f248-af36-4013-974b-c107257dcefd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vec{t} =\n",
    "\\begin{bmatrix}\n",
    "1 \\ \\ \\ x \\\\ 1 \\ \\ \\ x \\\\ \\vdots \\\\ 1 \\ \\ \\ x\n",
    "\\end{bmatrix}\n",
    "\\vec{w}\n",
    "+ \\vec{\\epsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74da242-8939-458d-9de2-18cc1b52eabf",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf99db2-9f01-4d3e-b92e-84803d2e823b",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b929050-89ab-46c5-94f2-7e00f247292c",
   "metadata": {},
   "source": [
    "OLS is a is a type of least squares method used in linear regression models as an estimator - it estimates the optimal values of the coefficients/weights vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e4516-dced-4967-9332-cebfb35f6a6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the Least Squares approach our goal is to minimize the  Residual Sum of Squares (RSS) given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddae1c7-a820-47a2-8b26-d4374ea3da4f",
   "metadata": {},
   "source": [
    "$$\\color{green}{ RSS =  \\sum{\\vec{\\epsilon} \\ ^2} = ||\\vec{\\epsilon}||^2}$$\n",
    "We can also write this as function of $\\vec{w}$ where X has row vectors $\\mathbf{\\vec{x}}$\n",
    "\n",
    "$$ S(\\vec{w}) = ||\\vec{t} - X\\vec{w}||^2 $$\n",
    "Expanding this we get\n",
    "$$ S(\\vec{w}) = (\\vec{t} - X\\vec{w})^T( \\vec{t} - X\\vec{w}) $$\n",
    "\n",
    "$$ = \\vec{t}^T\\vec{t}-\\vec{w}^T X^T \\vec{t} - \\vec{t}^TX\\vec{w}  +  \\vec{w}^T X^T X \\vec{w}$$\n",
    "Here $\\vec{w}^T X^T \\vec{t}$ is a scaler, and hence equal to it's own transpose. Noting that $(\\vec{w}^T X^T \\vec{t})^T = \\vec{t}^TX\\vec{w} $ we have\n",
    "\n",
    "$$ S(\\vec{w}) = \\vec{t}^T\\vec{t} - 2\\vec{t}^TX\\vec{w} + \\vec{w}^T X^T X \\vec{w} $$\n",
    "We know that $S(\\vec{w})$ has a minumum when its derivative with respect to $\\vec{w}$ is zero.\n",
    "\n",
    "$$ 0 = \\Delta S(\\vec{w}) = -X^T \\vec{t} + (X^TX)\\vec{w} $$\n",
    "Thus\n",
    "\n",
    "$$\\color{red}{ \\vec{w}_{OLS} = (X^TX)^{-1}X^T \\vec{t} }$$\n",
    "\n",
    "gives us the parameters which minimize the RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e1608-b9b2-4438-a833-a3463230afd3",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ab22b-d87b-4f5c-b921-523219554273",
   "metadata": {},
   "source": [
    "### Typeface vs Vector Notation\n",
    "From this point on I will be using typeface $\\mathbf{\\vec{x}}$ and $\\mathbf{t}$ to distinguish multiple observations from the data set as oppsosed to a single observation of a multivirate target. This will be easier to understand than using the vector notiation for both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80bb3f-a4af-4eff-80ec-ccf04672e7dd",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fee5bb-5757-4dd7-959c-d5e5d0376ff2",
   "metadata": {},
   "source": [
    "## Linear Basis Function Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedcfb0-f5d3-4af8-b1c1-844d3c6d76b0",
   "metadata": {},
   "source": [
    "We can extend our current model such that it incorporates non-linear functions of the regressors. Above we have assumed we are given N regressors and M sets of target values and regressors, so that X is in readable form (MxN). Here we assume the opposite, that we are given M regresors and N sets of data points.\n",
    "\n",
    "Thus we have $\\{t_n,\\vec{x_{0n}},\\vec{x_{2n}},....,\\vec{x_{(M-1)n}}\\}_{n=1}^{N}$, giving the prediction model\n",
    "$$ \\mathbf{t} = y(\\mathbf{\\vec{x}},\\vec{w}) + \\mathbf{\\epsilon}, \\ \\ \\ \\ where  \\ \\ y(\\mathbf{\\vec{x}},\\vec{w}) = \\sum_{j=0}^{M-1}w_j\\phi_j(\\mathbf{\\vec{x}})$$\n",
    "Let $\\vec{\\phi}(\\mathbf{\\vec{x}})$ be a vector of the basis functions $\\phi_j(\\mathbf{\\vec{x}})$ \n",
    "Then, we can write\n",
    "$$\\color{green}{\\mathbf{t} =\\vec{w}^T\\vec{\\phi}(\\mathbf{\\vec{x}}) + \\mathbf{\\epsilon}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2179760-2017-44f1-9d43-845454959602",
   "metadata": {},
   "source": [
    "In many practical applications, we will apply some form of fixed pre-processing. If the original variables comprise the vector $\\vec{x}$, then the features can be expressed in terms of the basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ee9fe-fbb1-4c49-8cc9-d8037d719a27",
   "metadata": {},
   "source": [
    "#### Polynomial Regression Example\n",
    "For instance, polynomial regression is given by the basis function $\\phi_j(\\mathbf{\\vec{x}}) = (\\vec{e_j}^T\\mathbf{\\vec{x}} \\ )^j$  , where $e_i$'s represent their respective elementry vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac295c0-16f4-43e1-84eb-b997afe32df9",
   "metadata": {},
   "source": [
    "## Maximum Likelihood (Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-brook",
   "metadata": {},
   "source": [
    "In maximum likelihood, our goal is to estimate the paramerters $\\vec{w}$ in order to maximize our likehood function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-vertex",
   "metadata": {},
   "source": [
    "Here we assume we that our target variable $\\mathbf{t}$ is given by a deterministic function with additive $Gaussian$ noise\n",
    "$$ \\color{green}{ \\mathbf{t} = y(\\mathbf{\\vec{x}},\\vec{w}) + \\mathbf{\\epsilon} =\\vec{w}^T\\vec{\\phi}(\\mathbf{\\vec{x}}) + \\mathbf{\\epsilon} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac6f57-220b-4dd8-80c2-86b069c9e4db",
   "metadata": {},
   "source": [
    "Since we have chosen the squared loss function $L(\\mathbf{t},y(\\mathbf{\\vec{x}},\\vec{w})) = \\{y(\\mathbf{\\vec{x}},\\vec{w}) - \\mathbf{t}\\}^2$ from below we have that\n",
    "\n",
    "$$E[t|\\vec{w},\\mathbf{\\vec{x}}] = y(\\mathbf{\\vec{x}},\\vec{w})$$\n",
    "\n",
    "gives us the optimal prediction for the new value of $\\mathbf{\\vec{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f400bf-9daf-4ca1-a057-77cb84be1086",
   "metadata": {},
   "source": [
    "Noting the zero mean Gaussian error term, and the fact it has precision (inverse variance) $\\beta$, for a single prediction of $t$ we have\n",
    "\n",
    "$$ \\color{green}{p(t|\\mathbf{\\vec{x}},\\vec{w},\\beta) = N(t \\ |y(\\mathbf{\\vec{x}},\\vec{w}), \\ \\beta^{-1}) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-clinic",
   "metadata": {},
   "source": [
    "Now, we turn to the case of predicting multiple value for the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-retail",
   "metadata": {},
   "source": [
    "Noting that $\\mathbf{t}$ is vector and $\\mathbf{\\vec{x}}$ is a matrix the $likelihood function$ is given by\n",
    "$$ \\color{green}{p(\\mathbf{t}|\\mathbf{\\vec{x}},\\vec{w},\\beta) = \\prod_{n=1}^N N(t_n \\ |\\vec{w}^T\\vec{\\phi}(\\vec{x_n}), \\ \\beta^{-1}) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb950d-50aa-4bd9-8837-48f7f2037091",
   "metadata": {},
   "source": [
    "Applying the Gaussian distribution formula\n",
    "$$ \\color{green}{p(\\mathbf{t}|\\mathbf{\\vec{x}},\\vec{w},\\beta) =\\prod_{n=1}^N \\dfrac{1}{(2\\pi\\beta^{-1})^{1/2}} * exp\\{ \\ -\\dfrac{1}{2\\beta^{-1}} (t_n - \\vec{w}^T \\vec{\\phi}(\\vec{x_n}) \\ )^2 \\ \\}  }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88d9df-f1e2-4fb4-b098-c93cf9f16b20",
   "metadata": {},
   "source": [
    "Taking the natural log of this gives us\n",
    "$$\n",
    "ln \\ p(\\mathbf{t}|\\mathbf{\\vec{x}},\\vec{w},\\beta) =\\sum^Nln((2\\pi\\beta^{-1})^{-1/2}) - \\ \n",
    "\\dfrac{1}{2\\beta^{-1}}\\sum^N(t_n - \\vec{w}^T \\vec{\\phi}(\\vec{x_n}) \\ )^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818a5ce-9dd2-4558-b72d-41f2892aad97",
   "metadata": {},
   "source": [
    "So that we have\n",
    "$$\\color{red}{ln \\ p(\\mathbf{t}|\\mathbf{\\vec{x}},\\vec{w},\\beta) = - \\dfrac{N}{2}ln(2\\pi) - \\dfrac{N}{2}ln(\\beta^{-1})  - \\ \n",
    "\\dfrac{\\beta}{2}\\sum^N(t_n - \\vec{w}^T \\vec{\\phi}(\\vec{x_n}) \\ )^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840eac8-a677-4402-a061-a5bbaff86b62",
   "metadata": {},
   "source": [
    "As we can see the last term in the formula is only the terms dependent on $\\vec{w}$, which happens to be the least squares error defined by\n",
    "$$\\color{red}{ S(\\vec{w}) = \\dfrac{1}{2}\\sum^N(t_n - \\vec{w}^T \\vec{\\phi}(\\vec{x_n}) \\ )^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569b6c7-fc33-4ef1-a0d5-4a2462b2aeb3",
   "metadata": {},
   "source": [
    "Hence, if we have an error term $\\epsilon$ that belongs to a  zero mean Gaussian distribution, then the least squares estimators are also the maximum likelihood estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432699fe-20c5-471a-b4ea-20f11bddb4cf",
   "metadata": {},
   "source": [
    "Taking the gradient of the log likelihood with respect to $\\vec{w}$ and setting it to zero in order to maximize the likelihood with respect to  $\\vec{w}$, we find that we only need to deal with the least squares error. So we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aadbc2d-a541-4cfc-96a8-0fd5be2b4ada",
   "metadata": {},
   "source": [
    "$$ 0 = \\Delta S(\\vec{w}) $$\n",
    "  $$ 0 = \\sum_{n=1}^N t_n\\vec{\\phi}(\\vec{x_n})^T - \\vec{w}^T \\sum_{n=1}^N \\vec{\\phi}(\\vec{x_n}) \\vec{\\phi}(\\vec{x_n})^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dbb386-7854-4b53-bd92-5352077a9d6c",
   "metadata": {},
   "source": [
    "If we let $\\Phi =\n",
    "\\begin{bmatrix}\n",
    "\\vec{\\phi}(\\vec{x_1})^T \\\\\n",
    "\\vec{\\phi}(\\vec{x_2})^T \\\\\n",
    "\\vdots \\\\\n",
    "\\vec{\\phi}(\\vec{x_N})^T\n",
    "\\end{bmatrix}\n",
    "$ then we can rewrite the equation as \n",
    "$$ 0 = \\Phi\\mathbf{t} - \\vec{w}^T\\Phi^T\\Phi $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf5eab7-9cf3-4119-a0db-d04318896e33",
   "metadata": {},
   "source": [
    "Thus we get the solution\n",
    "$$\\color{red}{ \\vec{w}_{ML} = (\\Phi^T\\Phi)^{-1}\\Phi^T\\mathbf{t} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c06e2a-7ffe-4d50-a7e8-0beaaf4c8b3d",
   "metadata": {},
   "source": [
    "Maximizing the log likelihood now with respect to $\\beta$ ,i.e. taking the gradient with respect to $\\beta$ and setting it to zero, gives us \n",
    "$$ - \\Delta \\dfrac{N}{2}ln(\\beta^{-1}) = \\Delta \\dfrac{\\beta}{2}\\sum^N(t_n - \\vec{w}^T \\vec{\\phi}(\\vec{x_n}) \\ )^2 $$ \n",
    "Note we have switched from $\\beta^{-1}$ to $\\beta$ by taking out the negative from the log.\n",
    "Now we have\n",
    "$$ \\dfrac{N}{1} * \\dfrac{1}{\\beta} = \\sum^N(t_n - \\vec{w}^T \\vec{\\phi}(\\vec{x_n}) \\ )^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdc8bc-18f3-46f0-a710-ab7c57b9c619",
   "metadata": {},
   "source": [
    "Thus we get the solution\n",
    "$$\\color{red}{\\beta_{ML} ^{-1} = \\dfrac{1}{N}\\sum^N(t_n - \\vec{w}^T \\vec{\\phi}(\\vec{x_n}) \\ )^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a7220-681b-4df8-8b85-c8c6e541501d",
   "metadata": {},
   "source": [
    "Now combining the soltions $\\vec{w}_{ML}$ and $\\beta_{ML}^{-1}$ we can use the following likelihood for $\\mathbf{t}$ \n",
    "$$\\color{red}{ p(\\mathbf{t}|\\mathbf{\\vec{x}},\\vec{w_{ML}},\\beta_{ML}^{-1}) = \\prod_{n=1}^N N(t_n \\ |y(\\vec{x_n},\\vec{w}_{ML}), \\ \\beta_{ML}^{-1})  }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f757a59-3055-4fe8-b459-ec110bb93010",
   "metadata": {},
   "source": [
    "### Bias Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d98ca-354b-4e32-b707-13414724753e",
   "metadata": {},
   "source": [
    "So far we have assumed that the bias parameter $w_0$ is implicit by including it in the rest of $\\vec{w}$ and by $\\vec{\\phi}$ having a $\\phi_0$ function. If we make the bias parameter explicit, then our error function becomes\n",
    "$$ E(\\vec{w}) = \\dfrac{1}{2}\\sum^N\\{ \\ t_n - w_0 - \\sum_{j=1}^{M-1}w_j{\\phi_j}(\\vec{x_n}) \\ \\}^2  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-bankruptcy",
   "metadata": {},
   "source": [
    "Setting the derivative with respect to $w_0$ equal to zero, and solving for $w_0$ , we obtain\n",
    "$$\\vec{w_0} = \\overline{t} -  \\sum_{j=1}^{M-1}w_j \\overline{\\phi_j}$$\n",
    "\n",
    "where\n",
    "$$ \\overline{t} = \\dfrac{1}{N}\\sum_{n=0}^{N} t_n  \\ \\ \\ \\ \\ and \\ \\ \\ \\ \\ \\overline{\\phi_j} = \\dfrac{1}{N}\\sum_{n=0}^{N} \\phi_j(\\vec{x_n})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-springer",
   "metadata": {},
   "source": [
    "Thus the bias $w_0$ compensates for the difference between the averages (over the training set) of the target values and the weighted sum of the averages of the basis function values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e262138-37e6-4354-990e-75462a4cc095",
   "metadata": {},
   "source": [
    "### Summary of the issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fc8b6-c054-48c2-a4c2-a33a19e4a7ac",
   "metadata": {},
   "source": [
    "In the maximum likelihood approach, the model complexitity, which is governed by the number of basis functions, needs to be controlled according to the size of the data set. In that approach, we can add a regulariztion term which allows us to control the model complexitiy using the regularization coefficient (learn later). The issue with then becomes how to decicde on the apporpriate complexiity for the particular problem. One solution is to utilize independent hold-out data to determine the complexitiy, but this both wastes valuable data and can be computationally expensive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd73b6-9192-4944-8eb3-7efa8bd8c73e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff693e-3a5c-4a0d-a9f6-7842e9115cd5",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7a6c1-e6b0-4d93-bdcf-fa05473224cf",
   "metadata": {},
   "source": [
    "We therefore turn to a Bayesian treatment of linear regression, which will lead to automatic methods of determining model complexity using the training data alone, and which will also avoid the over-fitting problem of maximum likelihood ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a63df-0e81-4ed9-9a4c-9a9cd90a9a07",
   "metadata": {},
   "source": [
    "We are given a specific form of Gaussian prior in order to simplify the treatment. Specifically, we consider a zero-mean isotropic Gaussian governed by a single precision parameter $\\alpha$ so that\n",
    "$$\\color{green}{p(\\vec{w}) = N(\\vec{w}|\\vec{0}, \\ \\alpha^{-1}\\mathbf{I})}$$\n",
    "with likelihood function\n",
    "$$\\color{green}{p(\\mathbf{t}|\\vec{w}) = N(\\mathbf{t}|\\mathbf{\\Phi} \\vec{w}, \\mathbf{\\beta}^{-1} ) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d5cda-6f0a-4f63-bc0d-f8cc503f1bae",
   "metadata": {},
   "source": [
    "and our posterior becomes\n",
    "$$\\color{green}{\n",
    "p(\\vec{w}|\\mathbf{t}) =\n",
    "N(\\vec{w}|\\mathbf{m}_{N}, \\mathbf{S}_{N})\n",
    "\\ \\ \\ \\ where  \\ \\ \\mathbf{m}_{N} = \\beta \\mathbf{S}_{N}\\mathbf{\\Phi}^T \\mathbf{t}, \\ \\mathbf{S}_{N}^{-1} = \\alpha\\mathbf{I}+ \\beta\\mathbf{\\Phi}^T\\mathbf{\\Phi}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844c7ec-5d01-4fd9-b2c7-416e9cdf28f0",
   "metadata": {},
   "source": [
    "The log of the posterior takes the form\n",
    "$$\n",
    "ln \\ p(\\vec{w}|\\mathbf{t}) = ln \\ p(\\mathbf{t}|\\vec{w}) + ln \\ p(\\vec{w})\n",
    "$$\n",
    "and viewed as function of $\\vec{w}$ we can write\n",
    "\n",
    "$$\n",
    "\\color{green}{\n",
    "ln \\ p(\\vec{w}|\\mathbf{t}) = -\\dfrac{\\beta}{2}\\sum_{n=1}^N \\{\\mathbf{t}_n - \\vec{w}^T\\vec{\\phi}(\\vec{x_n} \\ )\\}^2 - \\dfrac{\\alpha}{2} \\vec{w}^T\\vec{w} + const.}\n",
    "$$\n",
    "\n",
    "by using the log likelihood formula as well as $ln \\ p(\\vec{w}) = -\\dfrac{1}{2} \\vec{w}^T(\\alpha^{-1}\\mathbf{I})^{-1} \\vec{w} + const.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cfe905-a05c-4b61-8028-61604fa301cc",
   "metadata": {},
   "source": [
    "In practice, we are not usually interested in the value of $\\vec{w}$ itself but rather in making predictions of $t$ for new values of $\\mathbf{\\vec{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bef7a-8840-42a7-adb0-6b462138c187",
   "metadata": {},
   "source": [
    "This requires that we evalaute the $predictive \\ distribution$ defined by\n",
    "\n",
    "$$p(t|\\mathbf{t},\\alpha,\\beta) = \\int p(t,w|\\mathbf{t},\\alpha,\\beta) d\\vec{w}= \\int p(t|\\vec{w},\\mathbf{t},\\alpha,\\beta)p(\\vec{w}|\\mathbf{t},\\alpha,\\beta)d\\vec{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-accent",
   "metadata": {},
   "source": [
    "Here we have applied the following sum and product rules \n",
    "$$ p(t) = \\int p(t,\\vec{w})d\\vec{w}  = \\int p(t|\\vec{w})p(\\vec{w})d\\vec{w} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-paraguay",
   "metadata": {},
   "source": [
    "We can drop some of the notation if it irrelevant, so that we have\n",
    "$$\\color{green}{p(t) = \\int p(t|\\vec{w},\\beta)p(\\vec{w}|\\mathbf{t},\\alpha,\\beta)d\\vec{w}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-guyana",
   "metadata": {},
   "source": [
    "To solve this, we can use the Gaussian Bayes formulas, substituting in\n",
    "$$ p(t|\\vec{w})=N(t|\\mathbf{\\Phi}(\\mathbf{\\vec{x}})^T\\vec{w},\\beta^{-1})$$\n",
    "$$ p(\\vec{w}) =N(\\vec{w}|\\mathbf{m}_{N}, \\mathbf{S}_{N})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85217331-60a6-4816-835d-e1ccca2486cc",
   "metadata": {},
   "source": [
    "Note that we are solving\n",
    "$$p(t) = \\int N(t|\\mathbf{\\Phi}(\\mathbf{\\vec{x}})^T\\vec{w},\\beta^{-1})N(\\vec{w}|\\mathbf{m}_{N}, \\mathbf{S}_{N})d\\vec{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4293b9-3a5a-4163-a274-f4aff157ecaf",
   "metadata": {},
   "source": [
    "Thus, we have\n",
    "\n",
    "$$\\color{green}{p(t) = N(t|\\mathbf{\\Phi}(\\mathbf{\\vec{x}})^T\\mathbf{m}_{N},\\sigma^{2}_{N}(\\mathbf{\\vec{x}})) \\ \\ \\ \\ \\ where \\ \\ \\sigma^{2}_{N}(\\mathbf{\\vec{x}}) = \\beta^{-1}+\\mathbf{\\Phi}(\\mathbf{\\vec{x}})^T\\mathbf{S}_{N}\\mathbf{\\Phi}(\\mathbf{\\vec{x}})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433d330-95b5-46b4-ad59-97a9502add0f",
   "metadata": {},
   "source": [
    "## Loss functions for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-cooler",
   "metadata": {},
   "source": [
    "In estimating of the value of the target variable $t$ using $y(\\vec{x})$, suppose we incur a loss, which we define by a loss function $L(t,y(\\vec{x}))$.\n",
    "\n",
    "The average, or expected loss, is then given by\n",
    "$$\\color{green}{E[L] = \\int\\int L(t,y(\\vec{x})) \\ p(\\vec{x},t) \\ d\\vec{x} \\ dt}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1474d-b3e7-443d-8527-bdd1de734fb5",
   "metadata": {},
   "source": [
    "##### Squared Loss: $L(t,y(\\vec{x})) = \\{y(\\vec{x}) - t\\}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-bulletin",
   "metadata": {},
   "source": [
    "In this case, our expected loss is given by\n",
    "$$ \\color{green}{E[L] = \\int\\int \\{y(\\vec{x}) - t\\}^2 \\ p(\\vec{x},t) \\ d\\vec{x}dt}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-operation",
   "metadata": {},
   "source": [
    "Although I could approach this by minizing this function formally, by setting the gradient with respect to y(x) to 0, I will instead appraoch it informally, as it happens to be a bit more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-focus",
   "metadata": {},
   "source": [
    "We can manipulate our loss function so that we have\n",
    "$$ \\{y(\\vec{x}) - t\\}^2 = \\{y(\\vec{x}) + E[t|\\vec{x}] - E[t|\\vec{x}] - t\\}^2   \\ $$\n",
    "$$\\{y(\\vec{x}) - t\\}^2 = \\{y(\\vec{x}) - E[t|\\vec{x}]\\}^2 - 2\\{y(\\vec{x}) - E[t|\\vec{x}]\\}\\{ E[t|\\vec{x}] - t\\}+ \\{E[t|\\vec{x}] - t\\}^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-extra",
   "metadata": {},
   "source": [
    "Notice that \n",
    "$$ \\int \\int -2 \\{y(\\vec{x}) - E[t|\\vec{x}]\\}\\{E[t|\\vec{x}] - t\\}  \\ p(\\vec{x},t)  \\ d\\vec{x} \\ dt $$\n",
    "$$ = \\int \\int 2tp(\\vec{x},t)\\{y(\\vec{x}) - E[t|\\vec{x}]\\} - 2E[t|\\vec{x}]p(\\vec{x},t)\\{y(\\vec{x}) - E[t|\\vec{x}]\\} \\ d\\vec{x} \\ dt $$\n",
    "$$ = \\int t^2p(\\vec{x},t)\\{y(\\vec{x}) - E[t|\\vec{x}]\\} - t^2p(\\vec{x},t)\\{y(\\vec{x}) - E[t|\\vec{x}]\\} \\ d\\vec{x} $$\n",
    "$$ = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-vancouver",
   "metadata": {},
   "source": [
    "Hence our expected loss is given by\n",
    "$$E[L] = \\int \\{y(\\vec{x}) - E[t|\\vec{x}]\\}p(\\vec{x},t) \\  d\\vec{x} \\ + \\ \\int \\{E[t|\\vec{x}] - t\\}p(\\vec{x},t) \\ d\\vec{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-ratio",
   "metadata": {},
   "source": [
    "Now, notice two thing about the last term\n",
    "1. It is independent of $y(\\vec{x})$ so we cannot reduce it\n",
    "2. It is the expectation\\average of the $variance$ of the target distribution with respect to $\\vec{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-desktop",
   "metadata": {},
   "source": [
    "Thus, in order for our choice of $y(\\vec{x})$ to minimize $E[L]$, it must minimize the first term in the equation given above. Hence, we have\n",
    "$$\\color{red}{ y(\\vec{x}) = E[t|\\vec{x}] }$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
